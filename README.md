# AI in Cybersecurity â€” URV Group Research (Winter 2024, UMass Amherst)

## Overview
This repository documents our undergraduate research project conducted in Winter 2024 as part of the URV program at the College of Information and Computer Sciences (CICS), UMass Amherst. Working with the SPIN research group and mentored by a Ph.D. student, our team investigated the security risks and defenses associated with artificial intelligence (AI), focusing on generative AI and large-language models (LLMs).

We presented both a poster and an outline survey paper at the CICS Poster Day held at Manning.

## Project Team
- UMass Amherst Undergraduate Researchers: Jacqueline Bradley, Xingyu Cai, Turag Ikbal
- Ph.D. Mentor: Dayeon Kang (SPIN Research Group)

## Contents
- **Survey-Paper-Outline.pdf**  
  Literature review and structured survey on cyber offences, privacy risks, and mitigation strategies regarding generative AI and LLMs, including examples of jailbreaking, membership inference attacks, deepfakes, code vulnerabilities, and more [file:1].

- **AI_Cybersecurity.pdf**  
  Poster presentation summarizing offences/defenses, privacy leakage mechanisms, and recommended mitigation strategies; presented at CICS Poster Day [file:2].

## Summary
Our survey identifies key vulnerabilities in generative AI and LLMs, such as jailbreaking, model inversion, code generation risks, and data leakage, and highlights both attack vectors and emerging defense mechanisms including automated patching, access controls, and advanced privacy tools like OneShield Privacy Guard.  
Policy recommendations emphasize security-aware model development and improved regulatory oversight.

## Citation
If referencing this repository or its contents, please credit the URV program at CICS, UMass Amherst, and all student authors involved.

